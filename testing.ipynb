{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30935bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "# Dask\n",
    "from dask.distributed import Client, SSHCluster, progress\n",
    "import dask.array as da\n",
    "import dask.dataframe as dd\n",
    "# Others\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b960ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = SSHCluster(\n",
    "    [\"10.67.22.240\", \"10.67.22.17\", \"10.67.22.100\", \"10.67.22.126\"],\n",
    "    connect_options={\"known_hosts\": None},\n",
    "    worker_options={\"nthreads\": 2},\n",
    "    scheduler_options={\"port\": 0, \"dashboard_address\": \":8787\"}\n",
    ")\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93202f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cae1b74",
   "metadata": {},
   "source": [
    "## Developing the K-means|| algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f0ab6b",
   "metadata": {},
   "source": [
    "### Aux functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163cfb36",
   "metadata": {},
   "source": [
    "One important function here is the cost:\n",
    "\n",
    "$$\\phi_X(C) = \\sum_{x\\in X} d^2(x, C) = \\sum_{x\\in X} \\min_{i=1,..., k}||x-c_i||^2$$\n",
    "\n",
    "Notice that:\n",
    "\n",
    "$$||x-y||^2 = \\sum_j (x_j - y_j)^2 = \\sum_j x_j^2 - 2x_j y_j + y_j^2$$\n",
    "\n",
    "Then, for $x_n\\in\\{x_1, x_2, ..., x_{|X|}\\}$ and $c_m\\in{c_1, c_2, ..., c_{|C|}}$, we can define the squared distance matrix $D^2\\in\\mathbb{R}^{|X|\\times |C|}$ as:\n",
    "\n",
    "$$D^2_{nm} = ||x_n-c_m||^2 = \\sum_j x_{nj}^2 - 2x_{nj} c_{mj} + c_{mj}^2$$\n",
    "\n",
    "$$D^2_{nm} = \\sum_j x_{nj}x^T_{jn} - 2\\sum_j x_{nj} c^T_{jm} + \\sum_j c_{mj}c^T_{jm} = (XX^T)_{nn} - 2 (XC^T)_{nm} + (CC^T)_{mm}$$\n",
    "\n",
    "Furthermore, notice that if the data $X$ is fixed, then the matrix $(XX^T)$ is constant and does not depend on the choice of centroids $C$. Therefore, we may calculate the vector $(XX^T)_{nn}$ at the beggining of the process and store it so that we don't need to recalculate it every time we want to estimate the cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffbebd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_sample(data_path):\n",
    "    '''\n",
    "    Params:\n",
    "        data_path : str\n",
    "            Path to a given data file.\n",
    "    Output:\n",
    "        Returns the first row of the given data.\n",
    "    '''\n",
    "    # Read only first row using pandas\n",
    "    return pd.read_csv(data_path, nrows=1)\n",
    "\n",
    "\n",
    "def get_XXT_term(X):\n",
    "    '''\n",
    "    Params:\n",
    "        X : Dask array or dataframe\n",
    "            Array containing data points.\n",
    "    Output:\n",
    "        Returns the self multiplication term from the\n",
    "        squared distance matrix formula.\n",
    "    '''\n",
    "    # Turn into array\n",
    "    X_da = da.array(X)\n",
    "    # Get diagonal of X*X_T\n",
    "    XXT = da.einsum('ij,ij -> i', X_da, X_da)\n",
    "    return XXT\n",
    "\n",
    "\n",
    "def partial_squared_dist_matrix(C, X):\n",
    "    '''\n",
    "    Params:\n",
    "        C : Dask array or dataframe\n",
    "            Array containing centroid locations\n",
    "        X : Dask array or dataframe\n",
    "            Array containing data points.\n",
    "    Output:\n",
    "        Returns the partial squared distance matrix, \n",
    "        evaluated over the set of points X with respect \n",
    "        to the centroids C. The partial matrix is defined\n",
    "        as:\n",
    "         D'^2 = 2*XC^T + CC^T\n",
    "    '''\n",
    "    # Turn into arrays\n",
    "    X_da = da.array(X)\n",
    "    \n",
    "    # Calculate XC term\n",
    "    XC_term = da.einsum('nj, mj -> nm', X_da, C)\n",
    "\n",
    "    # Calculate CC term\n",
    "    CC_term = da.einsum('ij,ij -> i', C, C)\n",
    "    \n",
    "    return -2*XC_term + CC_term\n",
    "    \n",
    "\n",
    "def cost_function(C, X, XXT):\n",
    "    '''\n",
    "    Params:\n",
    "        C : Dask array or dataframe\n",
    "            Array containing centroid locations\n",
    "        X : Dask array or dataframe\n",
    "            Array containing data points.\n",
    "        XXT : Dask array or dataframe\n",
    "            Array with same number of rows as X, \n",
    "            containing the X*X^T term of the\n",
    "            squared distance matrix.\n",
    "    Output:\n",
    "        Returns the K-means cost function, evaluated\n",
    "        over the set of points X with respect to the\n",
    "        centroids C.\n",
    "    '''\n",
    "    # First, get partial squared distances\n",
    "    D2 = partial_squared_dist_matrix(C, X)\n",
    "\n",
    "    # Minimize over C axis and sum\n",
    "    D2_min_sum = da.sum(da.min(D2, axis=1))\n",
    "\n",
    "    # Add to XXT sum and return\n",
    "    return da.sum(XXT) + D2_min_sum\n",
    "\n",
    "\n",
    "def sample_new_centroids(C, X, XXT, phi, l):\n",
    "    '''\n",
    "    Params:\n",
    "        C : Dask array or dataframe\n",
    "            Array containing centroid locations\n",
    "        X : Dask array or dataframe\n",
    "            Array containing data points.\n",
    "        XXT : Dask array or dataframe\n",
    "            Array with same number of rows as X, \n",
    "            containing the X*X^T term of the\n",
    "            squared distance matrix.\n",
    "        phi : float\n",
    "            Current cost function for the data X\n",
    "            and the centroids C.\n",
    "        l : float or int\n",
    "            Oversampling factor, must be greater \n",
    "            than zero.\n",
    "    Output:\n",
    "        Returns new centroids from X, sampled with\n",
    "        probability:\n",
    "             p_x = l * D^2 / phi\n",
    "        where D^2 is the squared distance from x to\n",
    "        C and phi is the current cost function.\n",
    "    '''\n",
    "    # Get D^2\n",
    "    partial_D_sq = partial_squared_dist_matrix(C, X)\n",
    "    D_sq = da.add(XXT, da.min(partial_D_sq, axis=1))\n",
    "\n",
    "    # Get sampling probabilities\n",
    "    p_X = (l * D_sq / phi).compute_chunk_sizes()\n",
    "    \n",
    "    # Draw random numbers between 0 and 1\n",
    "    rand_nums = da.random.uniform(size=p_X.shape)\n",
    "\n",
    "    # Get new centroid indexes\n",
    "    C_prime_idx = rand_nums < p_X\n",
    "\n",
    "    # Gather new centroids from the data\n",
    "    C_prime = X.loc[C_prime_idx]\n",
    "    return C_prime\n",
    "\n",
    "def get_cluster_classification(C, X, XXT):\n",
    "    '''\n",
    "    Params:\n",
    "        C : Dask array or dataframe\n",
    "            Array containing centroid locations\n",
    "        X : Dask array or dataframe\n",
    "            Array containing data points.\n",
    "    Output:\n",
    "        Returns the corresponding centroid in C\n",
    "        for each sample in X.\n",
    "    '''\n",
    "    # Get partial squared distances\n",
    "    partial_D_sq = partial_squared_dist_matrix(C, X)\n",
    "\n",
    "    # Select the correct centroid from arg min\n",
    "    C_ids = da.argmin(partial_D_sq, axis=1)\n",
    "    return C_ids\n",
    "\n",
    "def get_centroid_weights(labels):\n",
    "    '''\n",
    "    Params:\n",
    "        labels : array\n",
    "            Set of labels indicating the centroid of\n",
    "            each data point.\n",
    "    Output:\n",
    "        Returns the weight of each centroid, defined \n",
    "        as the number of samples in the data closer\n",
    "        to that centroid than to any other centroid.\n",
    "    '''\n",
    "    unique_labels, counts = da.unique(da.array(labels), return_counts=True)\n",
    "    return unique_labels, counts\n",
    "\n",
    "\n",
    "def compute_centroids(X, weights, labels, dims):\n",
    "    '''\n",
    "    Params:\n",
    "        X : array\n",
    "            Array containing data points.\n",
    "        weights : array\n",
    "            Weight for each data point.\n",
    "        labels : array\n",
    "            Set of labels indicating the cluster of\n",
    "            each data point.\n",
    "        dims : int\n",
    "            Number of dimensions of the given data.\n",
    "    Output:\n",
    "        Returns the centroids for the given data and\n",
    "        the given partition.\n",
    "    '''    \n",
    "    # Turn data into Dask array\n",
    "    X = da.array(X)\n",
    "    weights = da.array(weights)\n",
    "    \n",
    "    # Get unique labels\n",
    "    unique_l = da.unique(da.array(labels)).compute_chunk_sizes()\n",
    "\n",
    "    # Init new centroids\n",
    "    C = da.array(np.zeros(shape=(len(unique_l), dims)))\n",
    "\n",
    "    # Operate for each unique cluster\n",
    "    for i, idx in enumerate(unique_l):\n",
    "        # Get data\n",
    "        C_data = X[labels == idx, :].compute_chunk_sizes()\n",
    "        data_weights = weights[labels == idx].compute_chunk_sizes()\n",
    "\n",
    "        # Calculate weighted mean as new centroid\n",
    "        new_C = da.average(C_data, axis=0, weights=data_weights)\n",
    "\n",
    "        # Store new centroid\n",
    "        C[i,:] += new_C\n",
    "        \n",
    "    return C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20116e7",
   "metadata": {},
   "source": [
    "### Main pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae80b4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script parameters\n",
    "RANDOM_SEED = 42\n",
    "INPUT_DATA = 'testing_data.csv'\n",
    "LABEL_COLUMN = 'label'\n",
    "NPARTITIONS = 1\n",
    "K = 3 # Number of clusters\n",
    "L = 3 # Oversampling factor\n",
    "VERBOSE = 2 # Amount of information to print out (0: Nothing, 1: Only timings, 2: All information)\n",
    "\n",
    "# Initialize random number generator from Dask and numpy seed\n",
    "rng = da.random.default_rng(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Read input data\n",
    "if VERBOSE == 2:\n",
    "    print('Reading input data\\n')\n",
    "data = pd.read_csv(INPUT_DATA)\n",
    "data_shape = data.shape\n",
    "\n",
    "# Separate labels from input\n",
    "if LABEL_COLUMN != None:\n",
    "    # Labels\n",
    "    future = client.scatter(data[LABEL_COLUMN])  # send labels to one worker\n",
    "    y_true = dd.from_delayed([future], meta=data[LABEL_COLUMN])  # build dask.dataframe on remote data\n",
    "    y_true = y_true.repartition(npartitions=NPARTITIONS).persist()  # split\n",
    "    client.rebalance(y_true)  # spread around all of your workers\n",
    "\n",
    "    # Input\n",
    "    X_width = data_shape[1]-1\n",
    "    X = data.drop(columns=[LABEL_COLUMN])\n",
    "    future = client.scatter(X) # send data to one worker\n",
    "    X = dd.from_delayed([future], meta=X)  # build dask.dataframe on remote data\n",
    "    X = X.repartition(npartitions=NPARTITIONS).persist()  # split\n",
    "    client.rebalance(X)  # spread around all of your workers\n",
    "    \n",
    "else:\n",
    "    # Only input\n",
    "    X_width = data_shape[1]\n",
    "    X = data\n",
    "    future = client.scatter(X) # send data to one worker\n",
    "    X = dd.from_delayed([future], meta=X, shape=data_shape)  # build dask.dataframe on remote data\n",
    "    X = X.repartition(npartitions=NPARTITIONS).persist()  # split\n",
    "    client.rebalance(X)  # spread around all of your workers\n",
    "\n",
    "# Run the K-means algorithm:\n",
    "# Get first sample as initial centroid\n",
    "first_sample = get_first_sample(INPUT_DATA)\n",
    "if LABEL_COLUMN != None:\n",
    "    first_sample = first_sample.drop(columns=[LABEL_COLUMN])\n",
    "C = da.array([np.array(first_sample).flatten()])\n",
    "\n",
    "# Calculate constant XXT term, \n",
    "# also persist since we are going to reuse it \n",
    "XXT = get_XXT_term(X).persist()\n",
    "\n",
    "# Get initial cost function\n",
    "phi_init = cost_function(C, X, XXT).compute()\n",
    "\n",
    "# Get number of iterations of the || algorithm\n",
    "O_log_phi = round(np.log(phi_init))\n",
    "\n",
    "# Init current cost\n",
    "phi = phi_init.copy()\n",
    "\n",
    "# Proceed with main || loop\n",
    "if VERBOSE == 2:\n",
    "    print('Running K-means|| initialization:')\n",
    "for i in range(O_log_phi):\n",
    "    if VERBOSE == 2:\n",
    "        print(f'Iteration {i+1} of {O_log_phi}')\n",
    "        \n",
    "    # Sample new centroids\n",
    "    C_prime = sample_new_centroids(C, X, XXT, phi, L)\n",
    "\n",
    "    # Add to the current centroids\n",
    "    C = da.vstack([C, C_prime]).compute()\n",
    "\n",
    "    # Calculate new cost and update current\n",
    "    phi = cost_function(C, X, XXT).compute()\n",
    "\n",
    "if VERBOSE == 2:\n",
    "    # Print number of final centroids from ||\n",
    "    print('\\nNumber of initialized centroids:', C.shape[0])\n",
    "    \n",
    "    # Print initial vs. final cost\n",
    "    print('Cost before initialization:', phi_init)\n",
    "    print('Cost after initialization:', phi)\n",
    "\n",
    "# Get the weight of each centroid\n",
    "if VERBOSE == 2:\n",
    "    print('\\nCalculating centroid weights')\n",
    "    \n",
    "X_labels = get_cluster_classification(C, X, XXT).compute_chunk_sizes()\n",
    "used_C, w_C = get_centroid_weights(X_labels)\n",
    "used_C = used_C.compute()\n",
    "w_C = w_C.compute()\n",
    "\n",
    "# Proceed with Lloyd's algorithm on the centroids\n",
    "if VERBOSE == 2:\n",
    "    print('\\nClustering centroids')\n",
    "\n",
    "# Initialize k final centroids, as the k-th heaviest\n",
    "# centroids from the previous step\n",
    "C_f = C[np.isin(w_C, np.sort(w_C, )[len(w_C)-K:])]\n",
    "\n",
    "# Calculate XXT for centroids\n",
    "CCT =  get_XXT_term(C).persist()\n",
    "\n",
    "# Perform iterative adjustments\n",
    "lloyd_done = False\n",
    "N_lloyd_steps = 0\n",
    "while not lloyd_done:\n",
    "    # Save old labels (after first iteration)\n",
    "    if N_lloyd_steps > 0:\n",
    "        old_labels = C_labels.copy()\n",
    "    \n",
    "    # Calculate current clustering\n",
    "    C_labels = get_cluster_classification(C_f, C, CCT).persist()\n",
    "\n",
    "    # Compute new centroids from mean within clusters\n",
    "    C_f = compute_centroids(C, w_C, C_labels, X_width).compute()\n",
    "    \n",
    "    # Check for termination condition (after first iteration)\n",
    "    if N_lloyd_steps > 0:\n",
    "        different_labels = da.sum(old_labels != C_labels).compute()\n",
    "        if different_labels == 0:\n",
    "            lloyd_done = True\n",
    "\n",
    "    # Increase step counter\n",
    "    N_lloyd_steps += 1\n",
    "\n",
    "if VERBOSE == 2:\n",
    "    print(f'Centroid clustering finished after {N_lloyd_steps} iterations.')\n",
    "\n",
    "# Compute final labels\n",
    "final_labels = get_cluster_classification(C_f, X, XXT).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5813e29e",
   "metadata": {},
   "source": [
    "## Closing the client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d181398",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
