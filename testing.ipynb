{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30935bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "# Dask\n",
    "from dask.distributed import Client, SSHCluster, progress\n",
    "import dask.array as da\n",
    "import dask.dataframe as dd\n",
    "# Others\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.datasets import fetch_rcv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b960ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = SSHCluster(\n",
    "    [\"10.67.22.240\", \"10.67.22.17\", \"10.67.22.100\", \"10.67.22.126\"],\n",
    "    connect_options={\"known_hosts\": None},\n",
    "    worker_options={\"nthreads\": 2},\n",
    "    scheduler_options={\"port\": 0, \"dashboard_address\": \":8787\"}\n",
    ")\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93202f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cae1b74",
   "metadata": {},
   "source": [
    "## Developing the K-means|| algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f0ab6b",
   "metadata": {},
   "source": [
    "### Aux functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163cfb36",
   "metadata": {},
   "source": [
    "One important function here is the cost:\n",
    "\n",
    "$$\\phi_X(C) = \\sum_{x\\in X} d^2(x, C) = \\sum_{x\\in X} \\min_{i=1,..., k}||x-c_i||^2$$\n",
    "\n",
    "Notice that:\n",
    "\n",
    "$$||x-y||^2 = \\sum_j (x_j - y_j)^2 = \\sum_j x_j^2 - 2x_j y_j + y_j^2$$\n",
    "\n",
    "Then, for $x_n\\in\\{x_1, x_2, ..., x_{|X|}\\}$ and $c_m\\in{c_1, c_2, ..., c_{|C|}}$, we can define the squared distance matrix $D^2\\in\\mathbb{R}^{|X|\\times |C|}$ as:\n",
    "\n",
    "$$D^2_{nm} = ||x_n-c_m||^2 = \\sum_j x_{nj}^2 - 2x_{nj} c_{mj} + c_{mj}^2$$\n",
    "\n",
    "$$D^2_{nm} = \\sum_j x_{nj}x^T_{jn} - 2\\sum_j x_{nj} c^T_{jm} + \\sum_j c_{mj}c^T_{jm} = (XX^T)_{nn} - 2 (XC^T)_{nm} + (CC^T)_{mm}$$\n",
    "\n",
    "Furthermore, notice that if the data $X$ is fixed, then the matrix $(XX^T)$ is constant and does not depend on the choice of centroids $C$. Therefore, we may calculate the vector $(XX^T)_{nn}$ at the beggining of the process and store it so that we don't need to recalculate it every time we want to estimate the cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffbebd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_sample(data_path):\n",
    "    '''\n",
    "    Params:\n",
    "        data_path : str\n",
    "            Path to a given data file.\n",
    "    Output:\n",
    "        Returns the first row of the given data.\n",
    "    '''\n",
    "    # Read only first row using pandas\n",
    "    return pd.read_csv(data_path, nrows=1)\n",
    "\n",
    "\n",
    "def get_XXT_term(X):\n",
    "    '''\n",
    "    Params:\n",
    "        X : Dask array or dataframe\n",
    "            Array containing data points.\n",
    "    Output:\n",
    "        Returns the self multiplication term from the\n",
    "        squared distance matrix formula.\n",
    "    '''\n",
    "    # Turn into array\n",
    "    X_da = da.array(X)\n",
    "    # Get diagonal of X*X_T\n",
    "    XXT = da.einsum('ij,ij -> i', X_da, X_da)\n",
    "    return XXT\n",
    "\n",
    "\n",
    "def partial_squared_dist_matrix(C, X):\n",
    "    '''\n",
    "    Params:\n",
    "        C : Dask array or dataframe\n",
    "            Array containing centroid locations\n",
    "        X : Dask array or dataframe\n",
    "            Array containing data points.\n",
    "    Output:\n",
    "        Returns the partial squared distance matrix, \n",
    "        evaluated over the set of points X with respect \n",
    "        to the centroids C. The partial matrix is defined\n",
    "        as:\n",
    "         D'^2 = 2*XC^T + CC^T\n",
    "    '''\n",
    "    # Turn into arrays\n",
    "    X_da = da.array(X)\n",
    "    \n",
    "    # Calculate XC term\n",
    "    XC_term = da.einsum('nj, mj -> nm', X_da, C)\n",
    "\n",
    "    # Calculate CC term\n",
    "    CC_term = da.einsum('ij,ij -> i', C, C)\n",
    "    \n",
    "    return -2*XC_term + CC_term\n",
    "    \n",
    "\n",
    "def cost_function(C, X, XXT):\n",
    "    '''\n",
    "    Params:\n",
    "        C : Dask array or dataframe\n",
    "            Array containing centroid locations\n",
    "        X : Dask array or dataframe\n",
    "            Array containing data points.\n",
    "        XXT : Dask array or dataframe\n",
    "            Array with same number of rows as X, \n",
    "            containing the X*X^T term of the\n",
    "            squared distance matrix.\n",
    "    Output:\n",
    "        Returns the K-means cost function, evaluated\n",
    "        over the set of points X with respect to the\n",
    "        centroids C.\n",
    "    '''\n",
    "    # First, get partial squared distances\n",
    "    D2 = partial_squared_dist_matrix(C, X)\n",
    "\n",
    "    # Minimize over C axis and sum\n",
    "    D2_min_sum = da.sum(da.min(D2, axis=1))\n",
    "\n",
    "    # Add to XXT sum and return\n",
    "    return da.sum(XXT) + D2_min_sum\n",
    "\n",
    "\n",
    "def sample_new_centroids(C, X, XXT, phi, l):\n",
    "    '''\n",
    "    Params:\n",
    "        C : Dask array or dataframe\n",
    "            Array containing centroid locations\n",
    "        X : Dask array or dataframe\n",
    "            Array containing data points.\n",
    "        XXT : Dask array or dataframe\n",
    "            Array with same number of rows as X, \n",
    "            containing the X*X^T term of the\n",
    "            squared distance matrix.\n",
    "        phi : float\n",
    "            Current cost function for the data X\n",
    "            and the centroids C.\n",
    "        l : float or int\n",
    "            Oversampling factor, must be greater \n",
    "            than zero.\n",
    "    Output:\n",
    "        Returns new centroids from X, sampled with\n",
    "        probability:\n",
    "             p_x = l * D^2 / phi\n",
    "        where D^2 is the squared distance from x to\n",
    "        C and phi is the current cost function.\n",
    "    '''\n",
    "    # Get D^2\n",
    "    partial_D_sq = partial_squared_dist_matrix(C, X)\n",
    "    D_sq = da.add(XXT, da.min(partial_D_sq, axis=1))\n",
    "\n",
    "    # Get sampling probabilities\n",
    "    p_X = (l * D_sq / phi).compute_chunk_sizes()\n",
    "    \n",
    "    # Draw random numbers between 0 and 1\n",
    "    rand_nums = da.random.uniform(size=p_X.shape)\n",
    "\n",
    "    # Get new centroid indexes\n",
    "    C_prime_idx = rand_nums < p_X\n",
    "\n",
    "    # Gather new centroids from the data\n",
    "    C_prime = X.loc[C_prime_idx]\n",
    "    return C_prime\n",
    "\n",
    "def get_cluster_classification(C, X, XXT):\n",
    "    '''\n",
    "    Params:\n",
    "        C : Dask array or dataframe\n",
    "            Array containing centroid locations\n",
    "        X : Dask array or dataframe\n",
    "            Array containing data points.\n",
    "    Output:\n",
    "        Returns the corresponding centroid in C\n",
    "        for each sample in X.\n",
    "    '''\n",
    "    # Get partial squared distances\n",
    "    partial_D_sq = partial_squared_dist_matrix(C, X)\n",
    "\n",
    "    # Select the correct centroid from arg min\n",
    "    C_ids = da.argmin(partial_D_sq, axis=1)\n",
    "    return C_ids\n",
    "\n",
    "def get_centroid_weights(labels):\n",
    "    '''\n",
    "    Params:\n",
    "        labels : array\n",
    "            Set of labels indicating the centroid of\n",
    "            each data point.\n",
    "    Output:\n",
    "        Returns the weight of each centroid, defined \n",
    "        as the number of samples in the data closer\n",
    "        to that centroid than to any other centroid.\n",
    "    '''\n",
    "    unique_labels, counts = da.unique(da.array(labels), return_counts=True)\n",
    "    return unique_labels, counts\n",
    "\n",
    "\n",
    "def compute_centroids(X, weights, labels, dims):\n",
    "    '''\n",
    "    Params:\n",
    "        X : array\n",
    "            Array containing data points.\n",
    "        weights : array\n",
    "            Weight for each data point.\n",
    "        labels : array\n",
    "            Set of labels indicating the cluster of\n",
    "            each data point.\n",
    "        dims : int\n",
    "            Number of dimensions of the given data.\n",
    "    Output:\n",
    "        Returns the centroids for the given data and\n",
    "        the given partition.\n",
    "    '''    \n",
    "    # Turn data into Dask array\n",
    "    X = da.array(X)\n",
    "    weights = da.array(weights)\n",
    "    \n",
    "    # Get unique labels\n",
    "    unique_l = da.unique(da.array(labels)).compute_chunk_sizes()\n",
    "\n",
    "    # Init new centroids\n",
    "    C = da.array(np.zeros(shape=(len(unique_l), dims)))\n",
    "\n",
    "    # Operate for each unique cluster\n",
    "    for i, idx in enumerate(unique_l):\n",
    "        # Get data\n",
    "        C_data = X[labels == idx, :].compute_chunk_sizes()\n",
    "        data_weights = weights[labels == idx].compute_chunk_sizes()\n",
    "\n",
    "        # Calculate weighted mean as new centroid\n",
    "        new_C = da.average(C_data, axis=0, weights=data_weights)\n",
    "\n",
    "        # Store new centroid\n",
    "        C[i,:] += new_C\n",
    "        \n",
    "    return C\n",
    "\n",
    "\n",
    "# Complete k-means pipeline\n",
    "def k_means_parallel(path, k, l, random_seed=None, label_column=None,\n",
    "                     datatype='dataframe', npartitions=1, chunk_size=1000,\n",
    "                     verbose=2):\n",
    "    '''\n",
    "    Params:\n",
    "        path : str\n",
    "            Path to the file containing the input data. If \"rcv1\", then\n",
    "            the RCV1 dataset is downloaded inside the function.\n",
    "        k : int\n",
    "            Number of clusters to use.\n",
    "        l : int or float\n",
    "            Oversampling factor for the K-means|| initialization method.\n",
    "        random_seed : int (optional)\n",
    "            Seed for the random number generators. Not used by default.\n",
    "        label_column : str (optional)\n",
    "            Name of the column containing the labels of the data. Not\n",
    "            used by default.\n",
    "        datatype : str (optional)\n",
    "            Either \"dataframe\" (default) or \"array\". Indicates how to\n",
    "            process the input data.\n",
    "        npartitions : int (optional)\n",
    "            Number of partitions to use if the data is processed as a\n",
    "            dataframe. Default is 1.\n",
    "        chunk_size : int (optional)\n",
    "            Chunk size to use if the data is processed as an array.\n",
    "            Default is 1000.\n",
    "        verbose : int (optional)\n",
    "            Amount of information to print. If 0, no information is \n",
    "            printed. If 1, only timing information is printed. If 2\n",
    "            (default), all information is printed.\n",
    "    Output:\n",
    "        Returns a dictionary with the following elements:\n",
    "            - centroids : array       -> Centroid positions\n",
    "            - cluster_labels : array  -> Cluster assignments\n",
    "            - timing : dict           -> Performance information\n",
    "    '''\n",
    "    # Begin overall timing\n",
    "    t1 = time.time()\n",
    "    \n",
    "    # Initialize random number generator from Dask and numpy seed\n",
    "    if random_seed != None:\n",
    "        rng = da.random.default_rng(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "\n",
    "    # Read input data\n",
    "    t_data = time.time()\n",
    "    if verbose == 2:\n",
    "        print('Reading input data')\n",
    "\n",
    "    if path not in ['rcv1']:\n",
    "        data = pd.read_csv(path)\n",
    "        data_shape = data.shape\n",
    "        \n",
    "    elif path == 'rcv1':\n",
    "        # Load RCV1 dataset\n",
    "        rcv1 = fetch_rcv1()\n",
    "        data = rcv1.data\n",
    "        data_shape = data.shape\n",
    "    dt_data = time.time() - t_data\n",
    "\n",
    "    # Separate labels from input\n",
    "    t_data_process = time.time()\n",
    "    if label_column != None:\n",
    "        # Labels\n",
    "        future = client.scatter(data[label_column])  # send labels to one worker\n",
    "        y_true = dd.from_delayed([future], meta=data[label_column])  # build dask.dataframe on remote data\n",
    "        y_true = y_true.repartition(npartitions=npartitions).persist()  # split\n",
    "        client.rebalance(y_true)  # spread around all of your workers\n",
    "    \n",
    "        # Input\n",
    "        X_width = data_shape[1]-1\n",
    "        X = data.drop(columns=[label_column])\n",
    "        future = client.scatter(X) # send data to one worker\n",
    "        X = dd.from_delayed([future], meta=X)  # build dask.dataframe on remote data\n",
    "        X = X.repartition(npartitions=npartitions).persist()  # split\n",
    "        client.rebalance(X)  # spread around all of your workers\n",
    "        \n",
    "    else:\n",
    "        # Only input\n",
    "        X_width = data_shape[1]\n",
    "        X = data\n",
    "        future = client.scatter(X) # send data to one worker\n",
    "        X = dd.from_delayed([future], meta=X, shape=data_shape)  # build dask.dataframe on remote data\n",
    "        X = X.repartition(npartitions=npartitions).persist()  # split\n",
    "        client.rebalance(X)  # spread around all of your workers\n",
    "        \n",
    "    dt_data_process = time.time() - t_data_process\n",
    "    if verbose > 0:\n",
    "        print(f'Data loaded and processed in {round(dt_data_process + dt_data, 1)} s')\n",
    "\n",
    "    # Run the K-means algorithm:\n",
    "    # Get first sample as initial centroid\n",
    "    t_first_centroid = time.time()\n",
    "    if path not in ['rcv1']:\n",
    "        first_sample = get_first_sample(path)\n",
    "    elif path == 'rcv1':\n",
    "        first_sample = data[0,:].toarray()\n",
    "        \n",
    "    if label_column != None:\n",
    "        first_sample = first_sample.drop(columns=[label_column])\n",
    "    C = da.array([np.array(first_sample).flatten()])\n",
    "\n",
    "    dt_first_centroid = time.time() - t_first_centroid\n",
    "    \n",
    "    # Calculate constant XXT term, \n",
    "    # also persist since we are going to reuse it \n",
    "    t_xxt = time.time()\n",
    "    XXT = get_XXT_term(X).persist()\n",
    "    dt_xxt = time.time() - t_xxt\n",
    "    \n",
    "    # Get initial cost function\n",
    "    t_phi_init = time.time()\n",
    "    phi_init = cost_function(C, X, XXT).compute()\n",
    "    dt_phi_init = time.time() - t_phi_init\n",
    "    \n",
    "    # Get number of iterations of the || algorithm\n",
    "    O_log_phi = round(np.log(phi_init))\n",
    "    \n",
    "    # Init current cost\n",
    "    phi = phi_init.copy()\n",
    "\n",
    "    # Proceed with main || loop\n",
    "    t_parallel_init = time.time()\n",
    "    if verbose == 2:\n",
    "        print('\\nRunning K-means|| initialization:')\n",
    "    for i in range(O_log_phi):\n",
    "        if verbose == 2:\n",
    "            print(f'Iteration {i+1} of {O_log_phi}')\n",
    "            \n",
    "        # Sample new centroids\n",
    "        C_prime = sample_new_centroids(C, X, XXT, phi, L)\n",
    "    \n",
    "        # Add to the current centroids\n",
    "        C = da.vstack([C, C_prime]).compute()\n",
    "    \n",
    "        # Calculate new cost and update current\n",
    "        phi = cost_function(C, X, XXT).compute()\n",
    "    \n",
    "    if verbose == 2:\n",
    "        # Print number of final centroids from ||\n",
    "        print('\\nNumber of initialized centroids:', C.shape[0])\n",
    "        \n",
    "        # Print initial vs. final cost\n",
    "        print('Cost before initialization:', phi_init)\n",
    "        print('Cost after initialization:', phi)\n",
    "\n",
    "    dt_parallel_init = time.time() - t_parallel_init\n",
    "    if verbose > 0:\n",
    "        print(f'K-means|| initialization finished in {round(dt_parallel_init, 1)} s')\n",
    "    \n",
    "    # Get the weight of each centroid\n",
    "    if verbose == 2:\n",
    "        print('\\nCalculating centroid weights')\n",
    "    t_weight_calc = time.time()\n",
    "    X_labels = get_cluster_classification(C, X, XXT).compute_chunk_sizes()\n",
    "    used_C, w_C = get_centroid_weights(X_labels)\n",
    "    used_C = used_C.compute()\n",
    "    w_C = w_C.compute()\n",
    "\n",
    "    dt_weight_calc = time.time() - t_weight_calc\n",
    "    if verbose > 0:\n",
    "        print(f'Centroid weight calculation finished in {round(dt_weight_calc, 1)} s')\n",
    "\n",
    "    # Proceed with Lloyd's algorithm on the centroids\n",
    "    t_lloyd = time.time()\n",
    "    if verbose == 2:\n",
    "        print('\\nClustering centroids')\n",
    "    \n",
    "    # Initialize k final centroids, as the k-th heaviest\n",
    "    # centroids from the previous step\n",
    "    C_f = C[np.isin(w_C, np.sort(w_C, )[len(w_C)-K:])]\n",
    "    \n",
    "    # Calculate XXT for centroids\n",
    "    CCT =  get_XXT_term(C).persist()\n",
    "    \n",
    "    # Perform iterative adjustments\n",
    "    lloyd_done = False\n",
    "    N_lloyd_steps = 0\n",
    "    while not lloyd_done:\n",
    "        # Save old labels (after first iteration)\n",
    "        if N_lloyd_steps > 0:\n",
    "            old_labels = C_labels.copy()\n",
    "        \n",
    "        # Calculate current clustering\n",
    "        C_labels = get_cluster_classification(C_f, C, CCT).persist()\n",
    "    \n",
    "        # Compute new centroids from mean within clusters\n",
    "        C_f = compute_centroids(C, w_C, C_labels, X_width).compute()\n",
    "        \n",
    "        # Check for termination condition (after first iteration)\n",
    "        if N_lloyd_steps > 0:\n",
    "            different_labels = da.sum(old_labels != C_labels).compute()\n",
    "            if different_labels == 0:\n",
    "                lloyd_done = True\n",
    "    \n",
    "        # Increase step counter\n",
    "        N_lloyd_steps += 1\n",
    "    dt_lloyd = time.time() - t_lloyd\n",
    "    \n",
    "    if verbose == 2:\n",
    "        print(f'Centroid clustering finished after {N_lloyd_steps} iterations and {round(dt_lloyd)} s')\n",
    "    elif verbose == 1:\n",
    "        print(f'\\nLloyd algorithm finished in {round(dt_lloyd)} s')\n",
    "    \n",
    "    # Compute final labels\n",
    "    if verbose == 2:\n",
    "        print('\\nCalculating final labels')\n",
    "    t_final_labels = time.time()\n",
    "    final_labels = get_cluster_classification(C_f, X, XXT).compute()\n",
    "    dt_final_labels = time.time() - t_final_labels\n",
    "    if verbose > 0:\n",
    "        print(f'Final labels calculated in {round(dt_final_labels)} s')\n",
    "\n",
    "    # Finish timing\n",
    "    dt_total = time.time() - t1\n",
    "    if verbose > 0:\n",
    "        print(f'\\nProcess finished in {round(dt_total, 1)} s')\n",
    "\n",
    "    # Build performance info\n",
    "    timing = {'total': dt_total,\n",
    "              'data_input': dt_data,\n",
    "              'data_processing': dt_data_process,\n",
    "              'first_centroid': dt_first_centroid,\n",
    "              'xxt': dt_xxt,\n",
    "              'phi_init': dt_phi_init, \n",
    "              'parallel_init': dt_parallel_init, \n",
    "              'weight_calc': dt_weight_calc, \n",
    "              'lloyd' : dt_lloyd, \n",
    "              'final_labels': dt_final_labels}\n",
    "    \n",
    "    # Gather output info into a dict and return\n",
    "    output_info = {'centroids': C_f,\n",
    "                   'cluster_labels': final_labels,\n",
    "                   'timing': timing}\n",
    "    return output_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20116e7",
   "metadata": {},
   "source": [
    "### Main pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d01aaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing on our curstom data:\n",
    "# Script parameters\n",
    "RANDOM_SEED = 42\n",
    "INPUT_DATA = 'testing_data.csv'\n",
    "LABEL_COLUMN = 'label'\n",
    "NPARTITIONS = 1\n",
    "K = 3 # Number of clusters\n",
    "L = 3 # Oversampling factor\n",
    "VERBOSE = 2 # Amount of information to print out (0: Nothing, 1: Only timings, 2: All information)\n",
    "\n",
    "# Run k-means\n",
    "test_res = k_means_parallel(INPUT_DATA, K, L, random_seed=RANDOM_SEED, label_column=LABEL_COLUMN,\n",
    "                     datatype='dataframe', npartitions=NPARTITIONS, verbose=VERBOSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5813e29e",
   "metadata": {},
   "source": [
    "## Closing the client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d181398",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
